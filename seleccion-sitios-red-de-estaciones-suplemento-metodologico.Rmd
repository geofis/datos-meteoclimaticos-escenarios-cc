---
title: "Selección de sitios para el establecimiento de una red de estaciones meteoclimáticas usando decisión multicriterio. Suplemento metodológico"
author: "José Martínez<br>Michela Izzo"
output:
  # bookdown::github_document2:
  #   number_sections: false
  #   fig_caption: yes
  bookdown::html_document2:
    number_sections: false
    code_folding: hide
    fig_caption: yes
    md_extensions: "-fancy_lists"
editor_options: 
  chunk_output_type: console
always_allow_html: true
references: ref/biblio.bib
bibliography: ref/biblio.bib
---

```{r supsetup, include=FALSE}
knitr::opts_chunk$set(
  cache = F, 
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  out.width = '100%',
  dpi = 300)
# options(digits = 3)
```

`r if(knitr::opts_knit$get("rmarkdown.pandoc.to") == 'gfm-yaml_metadata_block') 'Versión HTML (más legible e interactiva), [aquí](https://geofis.github.io/datos-meteoclimaticos-escenarios-cc/seleccion-sitios-red-de-estaciones-suplemento-metodologico.html)'`

`r if(knitr::opts_knit$get("rmarkdown.pandoc.to") == 'latex') 'Versión HTML (quizá más legible), [aquí](https://geofis.github.io/datos-meteoclimaticos-escenarios-cc/seleccion-sitios-red-de-estaciones-suplemento-metodologico.html)'`


## Procedimiento

Aplicamos una secuencia de tres técnicas interdependientes para formular distintas alternativas de redes de observación meteoclimático. En primer lugar, aplicamos un **proceso analítico jerárquico (AHP)**. Posteriormente, los resultados del AHP fueron usados como entrada de una simple **exclusión por factores limitantes**, específicamente excluir hexágonos por su localización respecto de accesos y cuerpos de agua. Finalmente, al resultado del análisis anterior, le aplicamos un **análisis de vecindad entre estaciones (existentes y propuestas)** con el objetivo de garantizar que las alternativas fuesen homogéneas en términos de distribución espacial.

## Aplicación del método AHP

### Paquetes y funciones

```{r suppaquetes}
library(raster)
library(psych)
library(kableExtra)
library(tidyverse)
library(ahpsurvey)
library(janitor)
estilo_kable <- function(df, titulo = '', cubre_anchura = T) {
  df %>% kable(format = 'html', escape = F, booktabs = T, digits = 2, caption = titulo) %>%
  kable_styling(bootstrap_options = c("hover", "condensed"), full_width = cubre_anchura)
}
```

### Escalas de valoración y matriz de comparación por parejas

```{r supvariables}
variables <- c(
    acce = "distancia a accesos",
    temp = "estacionalidad térmica",
    pluv = "estacionalidad pluviométrica",
    habi = "heterogeneidad de hábitat",
    agua = "distancia a cuerpos de agua",
    pend = "pendiente",
    inso = "horas de insolación",
    elev = "elevación"
)
col_ord <- as.vector(sapply(as.data.frame(combn(names(variables), 2)), paste0, collapse = '_'))
```

El método AHP consiste en descomponer un problema complejo en una estructura jerárquica de criterios y subcriterios, que consisten en variables o atributos del terreno en nuestro caso, y luego comparar las alternativas en función de cada uno de estos criterios. Los criterios se comparan en parejas (o pares, comparación pareada), asignando un valor numérico a la importancia relativa de cada criterio en relación con los demás. La evaluación pareada se realiza para cada par único de variables; así, el número de comparaciones posibles es $\frac{N(N-1)}{2}$

Originalmente, disponíamos de más de 100 variables espacializadas en el territorio dominicano para realizar nuestros análisis [@jose_ramon_martinez_batlle_2022_7367180], pero elegimos sólo ocho de ellas para ser ponderadas como criterios de selección por expertos, atendiendo al protocolo establecido en el método AHP. Los criterios seleccionados fueron *`r paste(as.vector(variables), collapse = ', ')`*. Elegimos estos ocho criterios por considerarlos relevantes según nuestro propio conocimiento de la problemática, y apoyándonos en estudios previos y recomendaciones de la Organización Meteorológica Mundial [@rojasbriceno2021]. Dado que comparamos ocho criterios en parejas, cada experto y experta realizó un total de $(8\times7)/2=28$ comparaciones.

Para evitar errores de redundancia y garantizar un diseño sistemático y eficiente, empleamos Formularios de Google al cual titulamos como ["Formulario de comparación pareada de criterios de identificación de sitios idóneos para una red de observación climática"](https://docs.google.com/forms/d/e/1FAIpQLScOx1bxW47LLEPQ_A6lHmSnpOQkUyHEoLJsRIKBNlbfQby5Dw/viewform?usp=sf_link). Programamos en Python las posibles comparaciones por pares y, seguidamente, a través de la API del Google Workspace, enviamos el diseño para su puesta en línea. Un total de nueve personas del área de climatología, análisis de datos y geografía física, rellenaron el formulario.

Para procesar los resultados de las consultas y generar una tabla de preferencias global con la cual construimos los pesos, en primer lugar generamos una tabla (no confundir con la matriz de comparación por parejas) donde cada columna es una comparación de dos atributos, por ejemplo A y B. Dado que la escala de valoración por parejas del método AHP, en sentido estricto, se apoya en ecuaciones lineales, en el fondo se utiliza una escala ordinal basada en un gradiente, que en el método original, usa números enteros 1 al 9. Esto significa que, al asignar "1", estamos indicando que los criterios comparados, por ejemplo, A y B, tienen la misma importancia. Del 2 al 9, el criterio B tiene mayor importancia, de manera creciente, que el A. Por otra parte, el grado de importancia de A sobre B se denota por medio de recíprocos `{1/2 , 1/3, ..., 1/8, 1/9}` y usando un gradiente inverso, es decir, la fracción más pequeña (`1/9`) indica mayor importancia relativa para el criterio A. Para denotar las valoraciones complementarias, el paquete `ahpsurvey` permite usar opuestos `{-2, -3, ..., -8, -9}`, que luego son recodificados a recíprocos en la matriz de comparación por parejas; preferiremos esta opción, es decir, usar opuestos, porque nos facilitó la recodificación con expresiones regulares.

Luego de recoger las valoraciones realizadas por medio de consultas en una tabla, el siguiente paso consistió en obtener la matriz de comparación por parejas, que tiene la siguiente forma:

$$ \mathbf{S_k} =\begin{pmatrix}
                      a_{1,1} & a_{1,2} & \cdots & a_{1,N} \\
                      a_{2,1} & a_{2,2} & \cdots & a_{2,N} \\
                      \vdots  & \vdots  & a_{i,j} & \vdots  \\
                      a_{N,1} & a_{N,2} & \cdots & a_{N,N}
                      \end{pmatrix} $$

donde $a_{i,j}$ representa la comparación del atributo $i$ y $j$. Tal como se ha comentado, si $i$ es más importante que $j$ en 6 unidades, $a_{i,j} = 6$ y $a_{j,i} = \frac{1}{6}$, es decir, el recíproco. Los datos de las comparaciones deben organizarse en esta forma matricial para realizar los análisis subsiguientes.

### Recodificación de valores y de nombres de columnas

En el estudio, utilizamos una escala modificada basada en sólo 7 posibles puntuaciones, recodificamos las puntuaciones de formulario de la siguiente manera: el valor 0 a 1 (usamos 0 en los formularios para facilitar la comprensión de la escala a los encuestados); los valores 33%, 66% y 100%, los distribuimos en el rango 2 a 9 de la siguiente manera:

```{r suptabequivalencias}
valor_formulario <- c('(100)', '(66)', '(33)', '0', '33', '66', '100')
recod_repartida <- FALSE
if(recod_repartida) {
    recodificado_ahp <- round(c(0-(2+3*((9-2)/3)), 0-(2+2*((9-2)/3)), 0-(2+((9-2)/3)),
                              1,
                              2+((9-2)/3), 2+2*((9-2)/3), 2+3*((9-2)/3)
                              ),
                            2)

} else {
  recodificado_ahp <- c(-9, -6, -3,
                        1,
                        3, 6, 9)
}
data.frame(
  `Valor en formulario` = valor_formulario,
  `Recodificado a escala AHP original` = recodificado_ahp,
  check.names = F) %>% 
  kable(format = 'html', escape = F, booktabs = T,
        caption = 'Tabla de recodificación de puntaciones de formulario a escala AHP original') %>%
  kable_styling(bootstrap_options = c("hover", "condensed"), full_width = T)
```

Por otro lado, creamos un "diccionario" (vector nombrado `variables`) de equivalencias entre los nombres largos de columnas de la tabla de resultados (que provienen escritas en el lenguaje natural de los formularios) y nombres cortos de cuatro caracteres. Este diccionario lo utilizamos para recodificar los nombres de las columnas de la tabla de respuestas a nombres cortos, con lo cual mejoramos la legibilidad de las representaciones gráficas y las impresiones de tablas y matrices de resultados.

```{r supimpresiontabequivalencias}
as.data.frame(variables) %>% 
  rownames_to_column() %>% 
  setNames(nm = c('Código', 'Nombre completo')) %>% 
  kable(format = 'html', escape = F, booktabs = T,
        caption = 'Tabla de equivalencias de nombres de las variables evaluadas') %>%
  kable_styling(bootstrap_options = c("hover", "condensed"), full_width = T)
```

Con la tabla de equivalencias de puntuaciones y el diccionario de nombres, recodificamos programáticas las respuestas obtenidas en los formularios a la escala original del método AHP, así como los nombres de columnas de la tabla de respuestas de comparación de atributos. En primer lugar, mostramos cómo realizamos la recodificación de puntuaciones.

La tabla de resultados de las puntuaciones en bruto (anonimizada), obtenida a partir del rellenado en Google Forms por parte de 9 consultados, se muestra a continuación.

```{r suptablaresultadosenbruto}
tabla_original <- read_csv('fuentes/respuestas-ahp/respuestas.csv')
tabla_en_bruto <- tabla_original[, -grep('Marca|Opcionalmente', colnames(tabla_original))]
tabla_en_bruto %>% 
    kable(format = 'html', escape = F, booktabs = T,
        caption = 'Tabla de resultados en bruto (anonimizada) obtenida a partir del rellenado del "Formulario de comparación pareada de criterios de identificación de sitios idóneos para una red de observación climática"') %>%
  kable_styling(bootstrap_options = c("hover", "condensed"), full_width = T)
```

Utilizamos una forma muy eficiente de recodificar, que consistió en aplicar expresiones regulares a las respuestas originales para extraer el valor de interés (e.g. "33"), y luego empleamos la función `match` para asociar dicha puntuación con su correspondiente valor en la escala AHP original.

```{r suptablaresultadosrecodificados}
tabla_recodificada <- sapply(
  tabla_en_bruto[, grep('^Valora.*', colnames(tabla_en_bruto))],
  function(x){
   sustituido <- gsub('(^[0-9]*|\\([0-9]*\\)):.*', '\\1', x)
   # paste(sustituido, '=', reescalado[match(sustituido, valor_formulario)]) #For testing
   recodificado_ahp[match(sustituido, valor_formulario)]
  })
tabla_recodificada %>% 
    kable(format = 'html', escape = F, booktabs = T,
        caption = 'Tabla de puntaciones recodificadas') %>%
  kable_styling(bootstrap_options = c("hover", "condensed"), full_width = T)
```

En segundo lugar, aplicamos la recodificación de nombres de columnas de la tabla de respuestas, que originalmente eran transcripciones de las preguntas del formulario de Google. Este paso nos ayudó a representar nombres más cortos en la tabla que posteriormente usamos como insumo (ver tabla \@ref(tab:suptablaresultadosparamatrizpareada)) para crear la matriz de comparación por parejas del método AHP.

```{r suptablaresultadosparamatrizpareada}
tabla_col_renom <- tabla_recodificada
cambiar_nombre_por_variable <- function(primera=T) {
  names(
    variables[match(
      gsub('(^.*variables )(.*?)( y )(.*$)',
           ifelse(primera, '\\2', '\\4'),
           colnames(tabla_col_renom)),
      variables)])
}
colnames(tabla_col_renom) <- paste0(
  cambiar_nombre_por_variable(),
  '_',
  cambiar_nombre_por_variable(primera = F)
)
tabla_col_renom %>% 
  kable(format = 'html', escape = F, booktabs = T,
        caption = 'Tabla de columnas renombradas (adaptada para la generación de  la matriz de comparación en parejas)') %>%
  kable_styling(bootstrap_options = c("hover", "condensed"), full_width = T)
```

El conjunto de datos de la tabla \@ref(tab:suptablaresultadosparamatrizpareada) recoge las respuestas dadas por las 9 personas consultada, cada una compuesta por 28 comparaciones en parejas de criterios (8 criterios). Analicemos algunos ejemplos para ilustrar el flujo seguido en la recodificación y los cambios de nombres de columnas de la tabla de resultados.

La primera fila contiene las valoraciones realizadas por la persona consultada número 1. En la primera pregunta, "*Valora la importancia relativa de las variables horas de insolación y elevación*", el consultado respondió "*33: Importancia moderada para elevación*" (ver tabla \@ref(tab:suptablaresultadosenbruto)). Dicha valoración fue recodificada a puntuaciones AHP con el valor `r as.vector(tabla_col_renom[1, 1])` (ver tabla \@ref(tab:suptablaresultadosrecodificados)); nótese que el valor recodificado es positivo, dado que el criterio que recibió la mayor importancia fue el que ocupaba la segunda posición en la pregunta.

Finalmente, tras realizar el renombrado, la columna en cuestión paso de nombrarse "*Valora la importancia relativa de las variables horas de insolación y elevación*" a `inso_elev` (ver tabla \@ref(tab:suptablaresultadosparamatrizpareada)). Esta cambio nos permitirá manejar atributos cortos en la matriz de comparación por parejas.

Ilustremos el uso del signo con otro ejemplo. Observemos la respuesta de la persona 1 a la octava pregunta ("*Valora la importancia relativa de las variables heterogeneidad de hábitat y horas de insolación*"). Notaremos que su respuesta fue "*(33): Importancia moderada para heterogeneidad de hábitat*", dando mayor importancia al criterio que ocupa la primera posición. Esta valoración se recodificó a `r as.vector(tabla_col_renom[1, 8])` (negativo) en la escala AHP, y la columna fue renombrada a `habi_inso`. La recodificación valores y de nombres de columnas es un paso crítico del AHP, porque es común la comisión de errores que terminan "colándose" hacia insumos del análisis. Es además el paso previo a la construcción de una matriz de comparación en parejas consistente, que es el insumo principal del AHP.

### Generación de la matriz de comparación en parejas

Este paso resultó relativamente fácil, puesto que en pasos posteriores se elaboraron los insumos que necesita la función `ahp.mat` del paquete `ahpsurvey`. Es importante remarcar una particularidad sobre el orden las columnas. El parámetro `atts` de la función `ahp.mat` debe contener un vector con los nombres de los atributos comparados, en nuestro caso, las 8 variables ya mencionadas. El orden de este vector es muy importante, pues la función `ahp.mat` espera que las columnas de la tabla fuente se encuentren en el siguiente orden: `atributo1_atributo2`, `atributo1_atributo3`, ..., `atributo1_atributo8`, `atributo2_atributo3`, `atributo2_atributo4`, ..., `atributo2_atributo8`, ..., `atributo7_atributo8`. Este objeto ya fue creado arriba mediante la función `combn`, y fue nombrado como `col_ord`. Por lo tanto, reordenaremos las columnas de la tabla de respuestas recodificadas usando dicho vector.

```{r supmatrizahp, results='asis'}
matriz_ahp <- tabla_col_renom[, col_ord] %>%
  ahp.mat(atts = names(variables), negconvert = TRUE)
map(matriz_ahp,
  ~ kable(x = .x, format = 'html', escape = F, booktabs = T, digits = 2) %>%
  kable_styling(bootstrap_options = c("hover", "condensed"), full_width = T)
)
```

Como primera evaluación de la calidad de la matriz de comparación en parejas, calculamos las preferencias de ponderación individuales de los consultados, para generar una tabla resumen con los pesos de preferencia de cada consultado. Este cálculo se realiza normalizando las matrices para que todas las columnas sumen 1, y luego se calculan los promedios por filas como los pesos de preferencia de cada atributo. Los promedios se pueden obtener de 4 formas posibles: media aritmética, media geométrica, media cuadrática y vector propio (*eigen vector*).

Usando las diferencias de los promedios de preferencias individuales, evaluamos las diferencias máximas entre métodos, como forma indirecta de determinar si existe consistencia en las valoraciones dadas por cada personas consultada; diferencias máximas mayores de 0.05 se consideran, a priori, dignas de escrutinio posterior (ver figura \@ref(fig:supdiferenciasmaximas)).

```{r supdiferenciasmaximas, fig.cap='Diferencias de los promedios de preferencias individuales entre los métodos "media aritmética" y "valor propio"'}
eigentrue <- ahp.indpref(matriz_ahp, atts = names(variables), method = "eigen")
geom <- ahp.indpref(matriz_ahp, atts = names(variables), method = "arithmetic")
error <- data.frame(id = 1:length(matriz_ahp), maxdiff = apply(abs(eigentrue - geom), 1, max))
error %>%
  ggplot(aes(x = id, y = maxdiff)) +
  geom_point() +
  geom_hline(yintercept = 0.05, linetype = "dashed", color = "red") +
  geom_hline(yintercept = 0, color = "gray50") +
  scale_x_continuous(breaks = seq_len(nrow(tabla_col_renom)), "ID de persona consultada") +
  scale_y_continuous("Diferencia máxima") +
  theme_minimal()
```

En este caso, la persona consultada número 3 parece haber aportado respuestas inconsistentes, por lo que este primer resultado, a priori, nos anima a revisar a fondo la consistencia de la matriz de comparación. A tal efecto, existen métricas específicas y mucho más robustas que el método de las diferencias mostrado arriba, para evaluar la consistencia de la matriz de comparación, como es por ejemplo la ratio o razón de consistencia $CR$, analizada en la próxima sección.

### Medición y representación de la consistencia de las respuestas

La métrica convencional para evaluar la consistencia de las respuestas aportadas por las personas consultadas es la razón de consistencia, la cual viene dada por la fórmula siguiente:

$$CR = \bigg(\frac{\lambda_{max}-n}{n-1}\bigg)\bigg(\frac{1}{RI}\bigg)$$

donde $CR$ es la razón de consistencia, $\lambda_{max}$ es el valor propio más grande del vector de comparación por parejas, $n$ es el número de atributos, en nuestro caso, $8$, y $RI$ es un índice aleatorio que puede ser provisto por el usuario a partir de simulaciones, que con el paquete `ahpsurvey` se puede generar mediante la función `ahp.ri`. El conjunto de $RI$ a continuación se generó a partir de `ahp.ri` con 500000 simulaciones (ver tabla \@ref(tab:suprisimuladospaqahpsurvey)), y están contenidas en la viñeta principal de la documentación del paquete `ahpsurvey` [@cho2019ahpsurvey]:

```{r suprisimuladospaqahpsurvey}
ri_sim <- t(data.frame(RI = c(0.0000000, 0.0000000, 0.5251686, 0.8836651, 1.1081014, 1.2492774, 1.3415514, 1.4048466, 1.4507197, 1.4857266, 1.5141022,1.5356638, 1.5545925, 1.5703498, 1.5839958)))
colnames(ri_sim) <- 1:15
ri_sim %>%
  kable(format = 'html', escape = F, booktabs = T, digits = 2,
        caption = 'Índices aleatorios generados por la función ahp.ri con 500000 simulaciones para 1 a 15 atributos') %>%
  kable_styling(bootstrap_options = c("hover", "condensed"), full_width = T)
```

Para fines de impresión, la tabla sólo muestra dos dígitos, pero en el caso concreto de 8 atributos, el $RI$ a usar sería `r round(ri_sim[8],7)`. Adicionalmente, comprobamos este resultado por nuestra cuenta. Para ello usamos la función `ahp.ri` y generamos un $RI$ con 10000 simulaciones para 8 atributos (argumento `dim` de la referida función), fijando la aleatorización en el número 99.

```{r supprobandori}
tiempo_10k <- system.time(probandoRI <- ahp.ri(nsims = 10000, dim = 8, seed = 99))
```

El tiempo de cómputo fue relativamente pequeño (\~ `r round(tiempo_10k[[3]], 0)` segundos) y el resultado para $RI$ es `r round(probandoRI, 7)`, el cual se aproxima bastante al generado por @cho2019ahpsurvey (tabla \@ref(tab:suprisimuladospaqahpsurvey)). Si generásemos un $RI$ con 500000 simulaciones, nos tomaría al menos un minuto y medio en una PC de altas prestaciones (o varios minutos en una PC común), y el resultado sería bastante parecido al mostrado por @cho2019ahpsurvey, por lo que nos parece conveniente usar este último ($RI=1.4048466$).

```{r supdefri}
RI <- ri_sim[8]
```

Con este índice aleatorio, calculamos la razón de consistencia `CR` de las respuestas aportadas por cada persona consultada, mediante la función `ahp.cr` aplicada a la matriz de comparación en parejas. La tabla \@ref(tab:suprazondeconsistencia) resume el cómputo de esta métrica.

```{r suprazondeconsistencia}
cr <- matriz_ahp %>% ahp.cr(atts = names(variables), ri = RI)
data.frame(`Persona consultada` = seq_along(cr), CR = cr, check.names = F) %>%
  estilo_kable(titulo = 'Razones de consistencia (consistency ratio) por persona consultada',
               cubre_anchura = F) %>% 
  kable_styling(position = 'left') %>% 
  column_spec(column = 1:2, width = "10em")
```

```{r supumbrales}
umbral_saaty <- 0.1
umbral_alterno <- 0.15
umbral <- ifelse(recod_repartida, umbral_alterno, umbral_saaty)
```

Saaty demostró que cuando el $CR$ es superior a un umbral rígido de `r umbral_saaty`, la elección se considera inconsistente [@saaty1977]. **En nuestro caso, elegimos el umbral de `r umbral` para** $CR$, por lo que obtuvimos un total de `r sum(cr<=umbral)` valoraciones consistentes (personas consultadas números `r paste(which(cr<=umbral), collapse=', ')`) y `r sum(cr>umbral)` inconsistentes (personas números `r paste(which(cr>umbral), collapse=', ')`) (comparar con tabla \@ref(tab:suprazondeconsistencia)).

```{r supnumconsistinconsist}
table(ifelse(cr <= umbral, 'Consistente', 'Inconsistente')) %>% as.data.frame() %>% 
  setNames(nm = c('Tipo', 'Número de cuestionarios')) %>% 
  estilo_kable(titulo = 'Número de cuestionarios según consistencia',
               cubre_anchura = F) %>% 
  kable_styling(position = 'left') %>% 
  column_spec(column = 1:2, width = "10em")
```

Calculamos también las preferencias o prioridades asignadas por cada persona consultada, así como la ponderación correspondiente (peso, valor propio dominante), mediante la función `ahp.indpref`, que proporciona una relación detallada. La visualización de los diagramas de cajas, gráficos de violín (que son gráficos de densidad en espejo acompañando al diagrama de cajas) y los puntos (*jitter*), todos superpuestos, resulta útil para visualizar la heterogeneidad de las ponderaciones de cada persona consultada por atributo. Aunque esta este tipo de gráfico es más conveniente para un número mayor de personas consultadas, la visualización con nuestros datos es bastante expresiva.

```{r supatributopesocrboxplot, fig.cap='Preferencias individuales por atributo y ratio de consistencia', out.width='100%'}
cr_indicador <- cr %>% 
  data.frame() %>% 
  mutate(`Persona consultada` = seq_along(cr), `CR indicador` = as.factor(ifelse(cr <= umbral, 1, 0))) %>%
  select(`CR indicador`, `Persona consultada`)

matriz_ahp %>% 
  ahp.indpref(names(variables), method = "eigen") %>% 
  mutate(`Persona consultada` = seq_along(cr)) %>%
  left_join(cr_indicador, by = 'Persona consultada') %>%
  gather(-matches('Persona|CR'), key = "var", value = "pref") %>%
  ggplot(aes(x = var, y = pref)) + 
  geom_violin(alpha = 0.6, width = 0.8, color = "transparent", fill = "gray") +
  geom_jitter(alpha = 0.6, height = 0, width = 0.1, aes(color = `CR indicador`)) +
  geom_boxplot(alpha = 0, width = 0.3, color = "#808080") +
  scale_x_discrete("Atributo", labels = stringr::str_wrap(variables[sort(names(variables))], width = 10)) +
  scale_y_continuous("Peso (valor propio dominante)", 
                     labels = scales::percent, 
                     breaks = c(seq(0,0.7,0.1))) +
  guides(color=guide_legend(title=NULL)) +
  scale_color_discrete(breaks = c(0,1),
                       type = c("#F8766D", "#00BA38"),
                       labels = c(paste("CR >", umbral), 
                                  paste("CR <", umbral))) +
  labs(NULL, caption = paste("n =", nrow(tabla_col_renom), ",", "CR promedio =",
                           round(mean(cr),3))) +
  theme_minimal() +
  theme(legend.position = 'bottom', axis.text.x = element_text(size = 7))
```

La figura \@ref(fig:supatributopesocrboxplot) resume las preferencias asignadas por las personas consultadas, con indicación de la consistencia de las mismas. Las dos estacionalidades, térmica y pluviométrica, así como las horas de insolación debidas a terreno y la elevación, reúnen la mayor parte de las preferencias de respuestas consistentes (puntos verdes); nótese que se han incluido las preferencias de respuestas inconsistentes también (puntos rojos). Por otra parte, los atributos que reciben menor ponderación son distancias a accesos y a cuerpos de agua, heterogeneidad de hábitat y pendiente.

Repitiendo el gráfico de preferencias individuales por atributo y ratio de consistencia, pero usando sólo las respuestas consistentes, obtenemos un claro patrón de preferencia por las dos estacionalidades, las horas de insolación y la elevación.

```{r supatributopesocrboxplotsolocons, fig.cap='Preferencias individuales por atributo y ratio de consistencia, sólo respuestas consistentes', out.width='100%'}
matriz_ahp[cr_indicador[,1]==1] %>% 
  ahp.indpref(names(variables), method = "eigen") %>% 
  mutate(`Persona consultada` = cr_indicador[cr_indicador[,1]==1, 'Persona consultada']) %>%
  inner_join(cr_indicador[cr_indicador[,1]==1,], by = 'Persona consultada') %>%
  gather(-matches('Persona|CR'), key = "var", value = "pref") %>%
  ggplot(aes(x = var, y = pref)) + 
  geom_violin(alpha = 0.6, width = 0.8, color = "transparent", fill = "gray") +
  geom_jitter(alpha = 0.6, height = 0, width = 0.1, color = "#00BA38") +
  geom_boxplot(alpha = 0, width = 0.3, color = "#808080") +
  scale_x_discrete("Atributo", labels = stringr::str_wrap(variables[sort(names(variables))], width = 10)) +
  scale_y_continuous("Peso (valor propio dominante)", 
                     labels = scales::percent, 
                     breaks = c(seq(0,0.7,0.1))) +
  guides(color=guide_legend(title=NULL)) +
  labs(NULL, caption = paste("n =", nrow(cr_indicador[cr_indicador[,1]==1,]),
                             ",", "CR promedio =",
                           round(mean(cr[cr_indicador[,1]==1]),3))) +
  theme_minimal() +
  theme(legend.position = 'bottom', axis.text.x = element_text(size = 7))
```

### Matriz de pesos

Para la aplicación de los criterios, primero obtuvimos la matriz de preferencias individuales y agregadas, conteniendo los pesos otorgados a cada criterio. Mediante el método de procesamiento abreviado mostrado a continuación, obtuvimos la matriz de pesos de acuerdo con el umbral de consistencia elegido, que en nuestro caso es `r umbral`. De esta manera, generamos una matriz de pesos sólo con las respuestas consistentes. Verificamos igualmente que la suma de los pesos sea igual a 1.

```{r supflujocompletoahp}
flujo_completo_ahp <- ahp(df = tabla_col_renom[, col_ord], 
                          atts = names(variables), 
                          negconvert = TRUE, 
                          reciprocal = TRUE,
                          method = 'arithmetic', 
                          aggmethod = "arithmetic", 
                          qt = 0.2,
                          censorcr = umbral,
                          agg = TRUE)
# sum(flujo_completo_ahp$aggpref[,1]) == 1
```

Las tablas \@ref(tab:prefind) y \@ref(tab:prefagg) muestran las preferencias individuales y agregadas, respectivamente, de las personas entrevistadas cuyas respuestas fueron consistentes. Las matriz agregada constituye el resultado principal del AHP, el cual aplicaremos a los criterios reclasificados. En las secciones siguientes, explicamos en detalle el procedimiento seguido para la reclasificación de criterios y la aplicación de los pesos obtenidos por el método AHP.

```{r prefind}
kable_prefind <- flujo_completo_ahp$indpref %>% 
  mutate(`Persona consultada` = cr_indicador[cr_indicador[,1]==1, 'Persona consultada']) %>% 
  relocate(`Persona consultada`) %>% 
  estilo_kable(titulo = 'Preferencias individuales',
               cubre_anchura = F) %>% 
  kable_styling(position = 'left') %>% 
  column_spec(column = 1:2, width = "10em")
kable_prefind
```

```{r prefagg}
prefagg <- flujo_completo_ahp$aggpref %>%
  as.data.frame() %>% 
  rename(`Preferencias agregadas` = AggPref, `Desviación estándar` = SD.AggPref) %>% 
  rownames_to_column('Variable') %>% 
  mutate(Variable = factor(Variable, labels = variables[sort(names(variables))])) %>% 
  arrange(desc(`Preferencias agregadas`))
kable_prefagg <- prefagg %>% 
  estilo_kable(titulo = 'Preferencias agregadas',
               cubre_anchura = F) %>% 
  kable_styling(position = 'left') %>% 
  column_spec(column = 1:2, width = "10em")
kable_prefagg
```

## Reclasificación de fuentes cartográficas: valores originales, intervalos, representación de la reclasificación

Utilizamos múltiples fuentes cartográficas como variables de territorio para modelizar la idoneidad de sitios candidatos para la instalación de estaciones meteoclimáticas. Las fuentes disponibles eran capas ráster servidas bajo distintas resoluciones y sistemas de referencia, por lo que fue necesario aplicar algoritmos de estadística zonal (reducción) y consolidar resultados en una geometría vectorial común. Para ello, redujimos todas las fuentes ráster al índice geoespacial de hexágonos H3 (*hex bins*) [@jose_ramon_martinez_batlle_2022_7367180]. Probamos distintas resoluciones de dicho índice, y tras algunas pruebas, elegimos la resolución "7". Con esta resolución, cubrimos el territorio dominicano (más un área de influencia) con aproximadamente 13,000 hexágonos de ca. 4$km^2$ cada uno. Dentro de cada hexágono, por medio de estadística zonal, obtuvimos la media de cada variable, la cual utilizamos como estadístico de referencia en la reclasificación descrita a continuación.

Reclasificamos los valores promedio de las ocho variables seleccionadas, aplicando criterios definidos por el equipo de investigación para cada criterio, para lo cual utilizamos conocimiento experto y referencias bibliográficas especializadas en redes de monitoreo meteoclimático, y adaptando las escalas a la realidad insular [@fao1976framework; @rojasbriceno2021]. A cada criterio, y para cada hexágono, asignamos un entero en una escala ordinal del 1 al 4 (de "marginalmente idóneo" a "altamente idóneo"). Para facilitar esta tarea, y garantizar reproducibilidad y consistencia, creamos funciones que realizaron la reclasificación de forma semitautomática. Representamos a continuación, para cada criterio, los valores originales mediante gráfico de violín, mostramos los intervalos usados en la reclasificación, y la representamos cartográficamente.


```{r funcionesfuentescartograficas}
source('R/funciones.R')
library(sf)
library(spdep)
library(kableExtra)
res_h3 <- 7 #Escribir un valor entre 4 y 7, ambos extremos inclusive
ruta_ez_gh <- 'https://raw.githubusercontent.com/geofis/zonal-statistics/'
# ez_ver <- 'da5b4ed7c6b126fce15f8980b7a0b389937f7f35/'
ez_ver <- 'd7f79365168e688f0d78f521e53fbf2da19244ef/'
ind_esp_url <- paste0(ruta_ez_gh, ez_ver, 'out/all_sources_all_variables_res_', res_h3, '.gpkg')
ind_esp_url
if(!any(grepl('^ind_esp$', ls()))){
  ind_esp <- st_read(ind_esp_url, optional = T, quiet = T)
  st_geometry(ind_esp) <- "geometry"
  ind_esp <- st_transform(ind_esp, 32619)
}
if(!any(grepl('^pais_url$', ls()))){
  pais_url <- paste0(ruta_ez_gh, ez_ver, 'inst/extdata/dr.gpkg')
  pais <- invisible(st_read(pais_url, optional = T, layer = 'pais', quiet = T))
  st_geometry(pais) <- "geometry"
  pais <- st_transform(pais, 32619)
}
if(!any(grepl('^ind_esp_inters$', ls()))){
  ind_esp_inters <- st_intersection(pais, ind_esp)
  colnames(ind_esp_inters) <- colnames(ind_esp)
  ind_esp_inters$area_sq_m <- units::drop_units(st_area(ind_esp_inters))
  ind_esp_inters$area_sq_km <- units::drop_units(st_area(ind_esp_inters))/1000000
}
if(!any(grepl('^ind_esp_inters$', ls())) && interactive()){
  print(ind_esp_inters)
}
```

Creación de objeto contenedor para la reclasificación de criterios.

```{r}
# Objeto que acogerá nombres de objetos
objetos <- character()
```

### Criterio 1. Distancia a accesos.

```{r osmdist}
objeto <- 'osm_rcl'
assign(
  objeto,
  generar_resumen_grafico_estadistico_criterios(
    variable = 'OSM-DIST mean',
    umbrales = c(50, 200, 500, 5000),
    nombre = variables[[1]],
    ord_cat = 'mim_rev')
)
get(objeto)[c('violin', 'mapa_con_pais')]
get(objeto)[['area_proporcional_kable']]
get(objeto)[['intervalos_y_etiquetas_kable']]
# clipr::write_clip(get(objeto)$intervalos_y_etiquetas)
if(!objeto %in% objetos) objetos <- c(objetos, objeto)
```

### Criterio 2. Estacionalidad térmica.

```{r estacionalidadtermica}
objeto <- 'tseasonizzo_rcl'
assign(
  objeto,
  generar_resumen_grafico_estadistico_criterios(
    variable = 'TSEASON-IZZO mean',
    umbrales = c(1.1, 1.3, 1.5),
    nombre = variables[[2]],
    ord_cat = 'mi')
)
get(objeto)[c('violin', 'mapa_con_pais')]
get(objeto)[['intervalos_y_etiquetas_kable']]
get(objeto)[['area_proporcional_kable']]
# clipr::write_clip(get(objeto)$intervalos_y_etiquetas)
if(!objeto %in% objetos) objetos <- c(objetos, objeto)
```

### Criterio 3. Estacionalidad pluviométrica.

```{r estacionalidadpluvio}
objeto <- 'pseasonizzo_rcl'
assign(
  objeto,
  generar_resumen_grafico_estadistico_criterios(
    variable = 'PSEASON-IZZO mean',
    umbrales = c(30, 40, 50),
    nombre = variables[[3]],
    ord_cat = 'mi')
)
get(objeto)[c('violin', 'mapa_con_pais')]
get(objeto)[['intervalos_y_etiquetas_kable']]
get(objeto)[['area_proporcional_kable']]
# clipr::write_clip(get(objeto)$intervalos_y_etiquetas)
if(!objeto %in% objetos) objetos <- c(objetos, objeto)
```

### Criterio 4. Heterogeneidad de hábitat.

```{r heterogeneidadhabitat}
objeto <- 'hethab_rcl'
assign(
  objeto,
  generar_resumen_grafico_estadistico_criterios(
    variable = 'GHH coefficient_of_variation_1km',
    umbrales = c(300, 450, 600),
    nombre = variables[[4]],
    ord_cat = 'im')
)
get(objeto)[c('violin', 'mapa_con_pais')]
get(objeto)[['intervalos_y_etiquetas_kable']]
get(objeto)[['area_proporcional_kable']]
# clipr::write_clip(get(objeto)$intervalos_y_etiquetas)
if(!objeto %in% objetos) objetos <- c(objetos, objeto)
```

### Criterio 5. Distancia a cuerpos de agua y humedales.

```{r cuerposaguadist}
objeto <- 'wbwdist_rcl'
assign(
  objeto,
  generar_resumen_grafico_estadistico_criterios(
    variable = 'WBW-DIST mean',
    umbrales = c(1000, 2000, 3000),
    nombre = variables[[5]],
    ord_cat = 'mi')
)
get(objeto)[c('violin', 'mapa_con_pais')]
get(objeto)[['intervalos_y_etiquetas_kable']]
get(objeto)[['area_proporcional_kable']]
# clipr::write_clip(get(objeto)$intervalos_y_etiquetas)
if(!objeto %in% objetos) objetos <- c(objetos, objeto)
```

### Criterio 6. Pendiente.

```{r pendiente}
objeto <- 'slope_rcl'
assign(
  objeto,
  generar_resumen_grafico_estadistico_criterios(
    variable = 'G90 Slope',
    umbrales = c(3, 9, 15),
    nombre = variables[[6]],
    ord_cat = 'im')
)
get(objeto)[c('violin', 'mapa_con_pais')]
get(objeto)[['intervalos_y_etiquetas_kable']]
get(objeto)[['area_proporcional_kable']]
# clipr::write_clip(get(objeto)$intervalos_y_etiquetas)
if(!objeto %in% objetos) objetos <- c(objetos, objeto)
```

### Criterio 7. Horas de insolación.

```{r}
objeto <- 'insol_rcl'
assign(
  objeto,
  generar_resumen_grafico_estadistico_criterios(
    variable = 'YINSOLTIME mean',
    umbrales = c(3900, 4100, 4300),
    nombre = variables[[7]],
    ord_cat = 'mi')
)
get(objeto)[c('violin', 'mapa_con_pais')]
get(objeto)[['intervalos_y_etiquetas_kable']]
get(objeto)[['area_proporcional_kable']]
# clipr::write_clip(get(objeto)$intervalos_y_etiquetas)
if(!objeto %in% objetos) objetos <- c(objetos, objeto)
```

### Criterio 8. Elevación.

```{r}
objeto <- 'ele_rcl'
assign(
  objeto,
  generar_resumen_grafico_estadistico_criterios(
    variable = 'CGIAR-ELE mean',
    umbrales = c(200, 400, 800),
    nombre = variables[[8]],
    ord_cat = 'mi')
)
get(objeto)[c('violin', 'mapa_con_pais')]
get(objeto)[['intervalos_y_etiquetas_kable']]
get(objeto)[['area_proporcional_kable']]
# clipr::write_clip(get(objeto)$intervalos_y_etiquetas)
if(!objeto %in% objetos) objetos <- c(objetos, objeto)
```

### Intervalos consolidados

Los intervalos o umbrales elegidos para definir las puntuaciones de criterios, están recogidos en la tabla (ver tabla \@ref(tab:umbralesapuntuaciones)).

```{r umbralesapuntuaciones}
puntuaciones_umbrales <- map(objetos, function(x) get(x)[['intervalos_y_etiquetas']] %>% 
  pivot_longer(cols = -matches('puntuación|etiquetas'), names_to = 'criterio') %>%
  mutate(criterio = gsub(' intervalos', '', criterio)) %>% 
  group_by(across(all_of(matches('etiquetas|criterio')))) %>% 
  summarise(value = paste(value, collapse = ' y ')) %>% 
  pivot_wider(names_from = contains('etiquetas'), values_from = value) %>% 
  select(criterio, `altamente idóneo`, `idóneo`, `moderadamente idóneo`, `marginalmente idóneo`)
) %>% bind_rows()
readODS::write_ods(puntuaciones_umbrales, 'fuentes/umbrales-criterios-ahp/puntuaciones.ods')
puntuaciones_umbrales_kable <- puntuaciones_umbrales %>% kable(format = 'html', escape = F, booktabs = T, digits = 2,
        caption = 'Puntuaciones de criterios para la selección de sitios de estaciones meteoclimáticas') %>%
  kable_styling(bootstrap_options = c("hover", "condensed"), full_width = T)
puntuaciones_umbrales_kable
```

### Áreas proporcionales consolidadas

Las áreas proporcionales consolidadas quedan recogidas en la tabla (ver tabla \@ref(tab:areasproporcionales)).

```{r areasproporcionales}
areas_proporcionales <- map(objetos, function(x) get(x)[['area_proporcional']] %>% 
  pivot_longer(cols = -matches('proporción'), names_to = 'criterio') %>%
  mutate(criterio = gsub(' etiquetas', '', criterio)) %>% 
  pivot_wider(names_from = value, values_from = proporción)) %>% bind_rows() %>% 
  select(criterio, `altamente idóneo`, `idóneo`, `moderadamente idóneo`, `marginalmente idóneo`) %>% 
  adorn_totals('col') 
readODS::write_ods(x = areas_proporcionales,
                   path = 'fuentes/umbrales-criterios-ahp/areas_proporcionales.ods')
areas_proporcionales_kable <- areas_proporcionales %>% kable(format = 'html', escape = F, booktabs = T, digits = 2,
        caption = 'Áreas proporcionales por cada criterios para la selección de sitios de estaciones meteoclimáticas') %>%
  kable_styling(bootstrap_options = c("hover", "condensed"), full_width = T)
areas_proporcionales_kable
```

### Representación consolidada de las reclasificaciones y puntuaciones agregadas

Unir los vectoriales de cada criterio y representar mapa.

```{r}
all_criteria <- map(objetos[2:length(objetos)], ~ get(.x)[['vectorial']] %>% st_drop_geometry) %>% 
  prepend(list(get(objetos[1])[['vectorial']])) %>% 
  reduce(left_join, by = "hex_id")
all_criteria %>% st_write('out/intervalos_etiquetas_puntuaciones_AHP_criterios_separados.gpkg', delete_dsn = T)
```

Mapas puntuaciones reclasificadas de cada criterio.

```{r, fig.width=8, fig.height=12}
paleta <- c("altamente idóneo" = "#018571", "idóneo" = "#80cdc1",
               "moderadamente idóneo" = "#dfd2b3", "marginalmente idóneo" = "#a6611a")
all_criteria_mapa <- all_criteria %>%
  select(all_of(contains('etiquetas'))) %>% 
  rename_with(~ stringr::str_replace(.x, 
                                       pattern = ' etiquetas', 
                                       replacement = ''), 
                matches('etiquetas')) %>% 
  pivot_longer(cols = -geometry) %>% 
  ggplot +
  aes(fill = value) +
  geom_sf(lwd=0) + 
  scale_fill_manual(values = paleta) +
  labs(title = paste('Reclasificación de valores de criterios')) +
  geom_sf(data = pais, fill = 'transparent', lwd = 0.5, color = 'grey50') +
  facet_wrap(~ name, ncol = 2) +
  theme_bw() +
  theme(
    legend.position = 'bottom',
    legend.key.size = unit(0.5, 'cm'), #change legend key size
    legend.key.height = unit(0.5, 'cm'), #change legend key height
    legend.key.width = unit(0.5, 'cm'), #change legend key width
    legend.title = element_blank(), #change legend title font size
    legend.text = element_text(size=5) #change legend text font size
    )
if(interactive()) dev.new()
all_criteria_mapa
```

Calculamos las puntuaciones agregadas  multiplicando las puntuaciones parciales de cada criterio por sus correspondientes pesos de preferencias agregadas, con lo cual generamos las puntuaciones agregadas. Estas puntuaciones las reclasificamos en las cuatro categorías habituales (marginalmente idóneo...altamente idóneo) siguiendo un criterio de número de desviaciones estándar.

```{r}
nombres_ahp_obj_sf <- data.frame(
  `Nombre objeto sf` = paste(variables, 'puntuación'),
  Etiqueta = variables, check.names = F) %>%
  rownames_to_column('Nombre AHP')
pesos <- flujo_completo_ahp$aggpref %>% as.data.frame %>%
  rownames_to_column('Nombre AHP') %>% 
  inner_join(nombres_ahp_obj_sf)
all_criteria_scores <- all_criteria %>%
  st_drop_geometry() %>% 
  select(all_of(c('hex_id', grep(' puntuación', colnames(all_criteria), value = T)))) %>%
  pivot_longer(-hex_id, names_to = 'Nombre objeto sf', values_to = 'Puntuación') %>% 
  inner_join(pesos %>% select(`Nombre objeto sf`, Etiqueta, peso=AggPref)) %>% 
  mutate(`Puntuación ponderada` = peso * `Puntuación`) %>% 
  group_by(hex_id) %>%
  summarise(`Puntuación agregada` = sum(`Puntuación ponderada`, na.rm = T)) %>%
  inner_join(all_criteria) %>% 
  st_sf(sf_column_name = 'geometry') %>% 
  mutate(`Puntuación agregada escalada` = scale(`Puntuación agregada`)[,1]) %>% 
  mutate(`Categoría agregada` = cut(`Puntuación agregada escalada`,
                                    breaks = c(min(`Puntuación agregada escalada`, na.rm = T),
                                               -1, 0, 1,
                                               max(`Puntuación agregada escalada`, na.rm = T)),
                                    labels = rev(names(paleta)),
                                    include.lowest = T)
  ) %>% 
  relocate(c(`Puntuación agregada escalada`, `Categoría agregada`), .after = `Puntuación agregada`)
if(interactive()) summary(all_criteria_scores$`Puntuación agregada`)
if(interactive()) table(all_criteria_scores$`Categoría agregada`)
all_criteria_scores %>% st_write('out/intervalos_etiquetas_puntuaciones_AHP_criterios_agregados.gpkg', delete_dsn = T)
```

Generamos una tabla de áreas proporcionales según categorías agregadas.

```{r}
areas_proporcionales_all_criteria <- all_criteria_scores %>% select(`Categoría agregada`) %>% 
  mutate(
    área = units::drop_units(st_area(geometry)),
    `área total` = sum(units::drop_units(st_area(geometry)))) %>%
  st_drop_geometry %>%
  group_by(`Categoría agregada`) %>%
  summarise(proporción = sum(área, na.rm = T)/first(`área total`)*100) %>%
  na.omit() %>%
  mutate(proporción = as.numeric(scale(proporción, center = FALSE,
                            scale = sum(proporción, na.rm = TRUE)/100)))
areas_proporcionales_all_criteria %>% 
    kable(format = 'html', escape = F, booktabs = T, digits = 2,
        caption = 'Áreas proporcionales de categorías agregadas para la selección de sitios de estaciones meteoclimáticas') %>%
      kable_styling(bootstrap_options = c("hover", "condensed"), full_width = T)
```

También generamos un mapa representando las categorías agregadas, el cual constituye el resultado final del análisis AHP.

```{r}
if(interactive()) dev.new()
all_criteria_scores_mapa <- all_criteria_scores %>% 
  ggplot +
  aes(fill = `Categoría agregada`) +
  geom_sf(lwd=0) + 
  scale_fill_manual(values = paleta) +
  # scale_fill_fermenter(palette = 'BrBG', direction = 1, breaks = c(-1, 0, 1)) +
  labs(title = paste('Categorías agregadas')) +
  geom_sf(data = pais, fill = 'transparent', lwd = 0.5, color = 'grey50') +
  theme_bw() +
  theme(
    legend.position = 'bottom',
    legend.key.size = unit(0.5, 'cm'), #change legend key size
    legend.key.height = unit(0.5, 'cm'), #change legend key height
    legend.key.width = unit(0.5, 'cm'), #change legend key width
    legend.title = element_blank(), #change legend title font size
    legend.text = element_text(size=5) #change legend text font size
    )
all_criteria_scores_mapa
```

## Exclusión por factores limitantes

Imputamos valores mínimos en las columnas `Puntuación agregada` y `Puntuación agregada escalada` a los hexágonos categorizados como "marginalmente idóneos" en el criterio de distancia a accesos o en el criterio de distancia a cuerpos de agua. Posteriormente, recalculamos la columna `Categorías agregadas` para que los hexágonos a los que se les imputaron valores, se reclasifiquen como "marginalmente idóneos".

```{r}
all_criteria_scores_excluded <- all_criteria_scores %>% 
  mutate(
    `Puntuación agregada` = ifelse(
      `distancia a accesos etiquetas` == 'marginalmente idóneo' | `distancia a cuerpos de agua etiquetas` == 'marginalmente idóneo',
      min(`Puntuación agregada`, na.rm = T), `Puntuación agregada`),
    `Puntuación agregada escalada` = ifelse(
      `distancia a accesos etiquetas` == 'marginalmente idóneo' | `distancia a cuerpos de agua etiquetas` == 'marginalmente idóneo',
      min(`Puntuación agregada escalada`, na.rm = T), `Puntuación agregada escalada`),
    `Categoría agregada` = cut(`Puntuación agregada escalada`,
                                    breaks = c(min(`Puntuación agregada escalada`, na.rm = T),
                                               -1, 0, 1,
                                               max(`Puntuación agregada escalada`, na.rm = T)),
                                    labels = rev(names(paleta)),
                                    include.lowest = T))
all_criteria_scores_excluded %>% st_write('out/intervalos_etiquetas_puntuaciones_AHP_criterios_agregados_excluded.gpkg', delete_dsn = T)
hexagonos_imputados <- sum(!(all_criteria_scores %>% pull(`Categoría agregada`) ==
                               all_criteria_scores_excluded %>% pull(`Categoría agregada`)))
```

Representamos el mapa.

```{r}
if(interactive()) dev.new()
all_criteria_scores_excluded_mapa <- all_criteria_scores_excluded %>% 
  ggplot +
  aes(fill = `Categoría agregada`) +
  geom_sf(lwd=0) + 
  scale_fill_manual(values = paleta) +
  # scale_fill_fermenter(palette = 'BrBG', direction = 1, breaks = c(-1, 0, 1)) +
  labs(title = paste('Categorías agregadas (exclusión por factores limitantes)')) +
  geom_sf(data = pais, fill = 'transparent', lwd = 0.5, color = 'grey50') +
  theme_bw() +
  theme(
    legend.position = 'bottom',
    legend.key.size = unit(0.5, 'cm'), #change legend key size
    legend.key.height = unit(0.5, 'cm'), #change legend key height
    legend.key.width = unit(0.5, 'cm'), #change legend key width
    legend.title = element_blank(), #change legend title font size
    legend.text = element_text(size=5) #change legend text font size
    )
all_criteria_scores_excluded_mapa
```

Y mostramos la distribución porcentual por categrías agregadas luego de la exclusión por factores limitantes.

```{r}
areas_proporcionales_all_criteria_excluded <- all_criteria_scores_excluded %>%
  select(`Categoría agregada`) %>% 
  mutate(
    área = units::drop_units(st_area(geometry)),
    `área total` = sum(units::drop_units(st_area(geometry)))) %>%
  st_drop_geometry %>%
  group_by(`Categoría agregada`) %>%
  summarise(proporción = sum(área, na.rm = T)/first(`área total`)*100) %>%
  na.omit() %>%
  mutate(proporción = as.numeric(scale(proporción, center = FALSE,
                            scale = sum(proporción, na.rm = TRUE)/100)))
areas_proporcionales_all_criteria_excluded %>% 
    kable(format = 'html', escape = F, booktabs = T, digits = 2,
        caption = 'Áreas proporcionales de categorías agregadas para la selección de sitios de estaciones meteoclimáticas con exclusión por factores limitantes') %>%
      kable_styling(bootstrap_options = c("hover", "condensed"), full_width = T)
```

## Escenarios

Para obtener los escenarios definitivos aplicamos un análisis de vecindad entre estaciones (existentes y propuestas), con el objetivo de evitar redundancia en la red.

Las EMC suelen colocarse a una distancia "óptima" entre sí, a efectos de garantizar mediciones precisas y representativas de las condiciones climáticas locales. La distancia entre EMC depende de varios factores, como el tamaño del área de estudio, el tipo de clima en la región y las variables climáticas específicas que se estén midiendo. En general, se intenta garantizar precisión y representatividad, evitando al mismo tiempo redundancia [@design1976hydrological].

A tal efecto, para implementar estos criterios, escribimos una función en R a la que denominamos `generar_centroides_distantes` que posteriormente aplicamos para obtener distintas alternativas de redes de EMC. Utilizando principalmente análisis de vecindad, la función necesita, como entrada, una geometría de puntos, o de polígonos individuales regulares (en nuestro caso, hexágonos del índice espacial H3), la cual se convierte en el área a cubrir sobre la cual se propondrá la nube de puntos. Adicionalmente, se debe especificar el número de puntos esperado. El algoritmo se encarga de distribuirlos lo más homogéneamente posible usando el mecanismo de la envolvente convexa. Alternativamente, se pueden especificar densidades o distancias de separación como argumentos, en lugar de fijar rígidamente un número específico de puntos de salida. La distribución espacial usada maximiza los criterios de precisión y representatividad, y se apega a los estándares sugeridos por la OMM [@design1976hydrological].

```{r}
escenarios <- c(100, 150, 250) #Cada estación debe cubrir dichas áreas en km2
n_esc <- length(escenarios)
```

Aplicamos la función definiendo `r n_esc`dos escenarios posibles, en los que cada estación cubre `r paste(escenarios[1:n_esc-1], collapse = ', ')` y `r escenarios[n_esc]` kilómetros cuadrados, respectivamente.

```{r}
# Categorías agregadas
categorias_elegidas <- c('altamente idóneo', 'idóneo')
names(categorias_elegidas) <- rep('categorías de idoneidad', length(categorias_elegidas))
# Criterio de separación (en este caso, kilómetros cuadrados por estación) 
names(escenarios) <- paste('Escenario:', escenarios, 'km2 por estación')
# Primero realizamos los cálculos
resumen_calculos_escenarios <- map(escenarios, 
    ~ generar_centroides_distantes(
      geom = all_criteria_scores_excluded %>%
        filter(`Categoría agregada` %in% categorias_elegidas),
      km2_por_puntos = .x, solo_calculos = T))
# Finalmente, creamos los objetos que exportaremos para visualización en SIG
escenarios_ai_mi <- map(
  escenarios,
  ~ generar_centroides_distantes(
    geom = all_criteria_scores_excluded %>%
      filter(`Categoría agregada` %in% categorias_elegidas),
    km2_por_puntos = .x))
# Mapas
escenarios_ai_mi_mapas <- map(names(escenarios_ai_mi),
    function(x) {
      escenarios_ai_mi[[x]] %>% ggplot + geom_sf(alpha = 0.8) +
        geom_sf(data = all_criteria_scores_excluded,
                aes(fill = `Categoría agregada`), lwd = 0, alpha = 0.4) +
        scale_fill_manual(values = paleta) +
        labs(title = x) +
        theme_bw()
    })
escenarios_ai_mi_mapas
# Exportar
map(names(escenarios_ai_mi),
    function(x) {
      sin_especiales <- iconv(names(escenarios_ai_mi[x]),
                              from = 'utf-8', to = 'ASCII//TRANSLIT')
      nombre_archivo <- tolower(paste0(gsub(' |: ', '_', sin_especiales), '.gpkg'))
      escenarios_ai_mi[[x]] %>% st_write(paste0('out/', nombre_archivo), delete_dsn = T)
    })
```

Comprobamos mediante distancias entre vecinos más próximos (en lo adelante, "el primer orden de vecindad") que se ha conseguido la separación idónea. La comprobación debe llevarnos al radio de un círculo promedio cuya área se aproxime a la densidad fijada en primera instancia. Probemos con el escenario de 100 kilómetros cuadrados por cada estación.

```{r}
map(escenarios_ai_mi, estadisticos_distancias_orden_1)
# Los valores de separación inicialmente esperados se obtuvieron
```

Comparando con la red existente, tomemos por ejemplo el caso de la de ONAMET. En el caso de dicha red, la distancia de separación entre estaciones activas para el primer orden de vecindad es superior a la distancia de separación entre sitios de nuestra propuesta, para cualquiera de los escenarios considerados. Además, hay al menos dos estaciones de la red de ONAMET que se encuentran separadas hasta a 48 km.

```{r}
onamet_para_vecindad <- st_read('out/con_indicacion_estatus_onamet.gpkg') %>% 
  filter(estado == 'activa')
estadisticos_distancias_orden_1(onamet_para_vecindad)
```

Si comparamos con la red de estaciones en estado "Bueno" del INDRHI, notaremos también un patrón similar: las estaciones se encuentran muy separadas entre sí.

```{r}
indrhi_para_vecindad_b <- st_read('out/con_indicacion_estatus_climaticas_indrhi.gpkg') %>% 
  filter(Estado == 'Bueno')
estadisticos_distancias_orden_1(indrhi_para_vecindad_b)
```

Igualmente, si comparamos con la misma red, pero incluyendo también las estaciones en estado "Regular", notaremos que igualmente las distancias de separación entre estaciones siguen siendo grandes.

```{r}
indrhi_para_vecindad_br <- st_read('out/con_indicacion_estatus_climaticas_indrhi.gpkg') %>% 
  filter(Estado == 'Bueno' | Estado == 'Regular')
estadisticos_distancias_orden_1(indrhi_para_vecindad_br)
```

Las estaciones de la red de ONAMET fueron caracterizadas recientemente en dos estados "activa" e "inactiva o no reportada". En nuestro análisis de vecindad para eliminar redundancia, consideramos únicamente las activas. Por otra parte, la red del INDRHI cuenta con estaciones que, en 2019 se encontraban en estado "Bueno", "Regular" y "Malo". Decidimos considerar tanto las estaciones en estado "Bueno" como las que se encontraban en estado "Regular". Así, para cada uno de nuestros escenarios de densidad (100, 150 y 250 $km^2$), eliminamos redundancia usando dos redes combinadas distintas: 1) ONAMET activas + INDRHI buenas, y 2) ONAMET activas + INDRHI buenas y regulares. De esta manera, como producto final, obtuvimos 6 escenarios distintos.

```{r}
rgdal::setCPLConfigOption("GDAL_PAM_ENABLED", "FALSE")
actbue <- c(indrhi_para_vecindad_b %>% st_geometry(),
                onamet_para_vecindad %>% st_geometry()) %>% st_transform(32619)
actbue_r <- rasterize(x = as(actbue, 'Spatial'),
                                      y = raster(extent(ind_esp),
                                                 resolution = 500, crs = 'EPSG:32619'),
                                      field = 1)
actbue_d <- distance(actbue_r)
actbue_d %>% writeRaster('out/onamet_indrhi_actbue_dist_500x500_distancia.tif',
                         overwrite = T, setStatistics = F)

actbuereg <- c(indrhi_para_vecindad_br %>% st_geometry(),
                onamet_para_vecindad %>% st_geometry()) %>% st_transform(32619)
actbuereg_r <- rasterize(x = as(actbuereg, 'Spatial'),
                                      y = raster(extent(ind_esp),
                                                 resolution = 500, crs = 'EPSG:32619'),
                                      field = 1)
actbuereg_d <- distance(actbuereg_r)
actbuereg_d %>% writeRaster('out/onamet_indrhi_actbuereg_dist_500x500_distancia.tif',
                            overwrite = T, setStatistics = F)
```

En el caso del escenario "100 $km^2$ por estación", la distancia de corte entre estaciones es de 11 km. Por lo tanto, eliminaremos las estaciones propuestas por nosotros que queden dentro de ese rango respecto de estaciones de ONAMET y/o INDRHI existentes.

### Escenario 100 km\textsuperscript{2} por estación, eliminando propuestas de sitios redundantes respecto de ONAMET activas + INDRHI buenas

- Mapa

```{r}
indice <- 1; escenario <- '100'
redundancia <- 'activas_buenas'; estaciones <- actbue; distancia <- actbue_d
esc_d <- raster::extract(distancia, escenarios_ai_mi[[indice]])
esc <- escenarios_ai_mi[[indice]] %>%
  mutate(dist_onamet_indrhi = esc_d) %>% 
  filter(dist_onamet_indrhi > resumen_calculos_escenarios[[indice]]$`Distancia esperada entre vecinos`*1000) %>%
  st_join(all_criteria_scores_excluded, left = T)
esc %>%
  st_write(paste0('out/escenario_', escenario, '_km2_por_estacion_exclusion_redundancia_', redundancia, '.gpkg'), delete_dsn = T)
obj <- paste0('esc_', escenario, '_', redundancia, '_mapa')
assign(obj,
       bind_rows(esc %>% st_geometry %>% st_as_sf() %>%
                   mutate(id='sitios propuestos'),
                 estaciones %>% st_geometry %>% st_as_sf() %>%
                   mutate(id='estaciones existentes')) %>%
         ggplot +
         geom_sf(data = pais, fill = 'transparent', color = 'grey50') +
         geom_sf(alpha = 0.8, aes(fill = id, shape = id), size = 1.5) +
         scale_fill_manual(values = c('grey70', 'black')) +
         scale_shape_manual(values = c(25, 21)) +
         # geom_sf(data = all_criteria_scores_excluded,
         #         aes(fill = `Categoría agregada`), lwd = 0, alpha = 0.4) +
         # scale_fill_manual(values = paleta) +
         labs(title = paste0(trimws(names(escenarios)[indice]),'\n',
                             'Eliminación de redundancia respecto de estaciones ',
                             gsub('_', '+', redundancia))) +
         theme_bw() + 
         ggspatial::annotation_scale(style = 'ticks') +
         theme(legend.title = element_blank())) 
get(obj) #Mapa
```

- Tabla-resumen conteniendo los valores

```{r}
obj <- paste0('esc_', escenario, '_', redundancia, '_df_resumen')
assign(obj,
       esc %>% st_drop_geometry %>%
         dplyr::select(`Categoría agregada`) %>% count(`Categoría agregada`) %>% 
         mutate(`Monto (US$)` = ifelse(`Categoría agregada` == 'idóneo', n*7000, n*35000)) %>% 
         adorn_totals())
get(obj) #Tabla
```

### Escenario 100 km\textsuperscript{2} por estación, eliminando propuestas de sitios redundantes respecto de ONAMET activas + INDRHI buenas y regulares

- Mapa

```{r}
indice <- 1; escenario <- '100'
redundancia <- 'activas_buenas_regulares'; estaciones <- actbuereg; distancia <- actbuereg_d
esc_d <- raster::extract(distancia, escenarios_ai_mi[[indice]])
esc <- escenarios_ai_mi[[indice]] %>%
  mutate(dist_onamet_indrhi = esc_d) %>% 
  filter(dist_onamet_indrhi > resumen_calculos_escenarios[[indice]]$`Distancia esperada entre vecinos`*1000) %>%
  st_join(all_criteria_scores_excluded, left = T)
esc %>%
  st_write(paste0('out/escenario_', escenario, '_km2_por_estacion_exclusion_redundancia_', redundancia, '.gpkg'), delete_dsn = T)
obj <- paste0('esc_', escenario, '_', redundancia, '_mapa')
assign(obj,
       bind_rows(esc %>% st_geometry %>% st_as_sf() %>%
                   mutate(id='sitios propuestos'),
                 estaciones %>% st_geometry %>% st_as_sf() %>%
                   mutate(id='estaciones existentes')) %>%
         ggplot +
         geom_sf(data = pais, fill = 'transparent', color = 'grey50') +
         geom_sf(alpha = 0.8, aes(fill = id, shape = id), size = 1.5) +
         scale_fill_manual(values = c('grey70', 'black')) +
         scale_shape_manual(values = c(25, 21)) +
         labs(title = paste0(trimws(names(escenarios)[indice]),'\n',
                             'Eliminación de redundancia respecto de estaciones ',
                             gsub('_', '+', redundancia))) +
         theme_bw() + 
         ggspatial::annotation_scale(style = 'ticks') +
         theme(legend.title = element_blank())) 
get(obj) #Mapa
```

- Tabla-resumen conteniendo los valores

```{r}
obj <- paste0('esc_', escenario, '_', redundancia, '_df_resumen')
assign(obj,
       esc %>% st_drop_geometry %>%
         dplyr::select(`Categoría agregada`) %>% count(`Categoría agregada`) %>% 
         mutate(`Monto (US$)` = ifelse(`Categoría agregada` == 'idóneo', n*7000, n*35000)) %>% 
         adorn_totals())
get(obj) #Tabla
```

### Escenario 150 km\textsuperscript{2} por estación, eliminando propuestas de sitios redundantes respecto de ONAMET activas + INDRHI buenas

- Mapa

```{r}
indice <- 2; escenario <- '150'
redundancia <- 'activas_buenas'; estaciones <- actbue; distancia <- actbue_d
esc_d <- raster::extract(distancia, escenarios_ai_mi[[indice]])
esc <- escenarios_ai_mi[[indice]] %>%
  mutate(dist_onamet_indrhi = esc_d) %>% 
  filter(dist_onamet_indrhi > resumen_calculos_escenarios[[indice]]$`Distancia esperada entre vecinos`*1000) %>%
  st_join(all_criteria_scores_excluded, left = T)
esc %>%
  st_write(paste0('out/escenario_', escenario, '_km2_por_estacion_exclusion_redundancia_', redundancia, '.gpkg'), delete_dsn = T)
obj <- paste0('esc_', escenario, '_', redundancia, '_mapa')
assign(obj,
       bind_rows(esc %>% st_geometry %>% st_as_sf() %>%
                   mutate(id='sitios propuestos'),
                 estaciones %>% st_geometry %>% st_as_sf() %>%
                   mutate(id='estaciones existentes')) %>%
         ggplot +
         geom_sf(data = pais, fill = 'transparent', color = 'grey50') +
         geom_sf(alpha = 0.8, aes(fill = id, shape = id), size = 1.5) +
         scale_fill_manual(values = c('grey70', 'black')) +
         scale_shape_manual(values = c(25, 21)) +
         # geom_sf(data = all_criteria_scores_excluded,
         #         aes(fill = `Categoría agregada`), lwd = 0, alpha = 0.4) +
         # scale_fill_manual(values = paleta) +
         labs(title = paste0(trimws(names(escenarios)[indice]),'\n',
                             'Eliminación de redundancia respecto de estaciones ',
                             gsub('_', '+', redundancia))) +
         theme_bw() + 
         ggspatial::annotation_scale(style = 'ticks') +
         theme(legend.title = element_blank())) 
get(obj) #Mapa
```

- Tabla-resumen conteniendo los valores

```{r}
obj <- paste0('esc_', escenario, '_', redundancia, '_df_resumen')
assign(obj,
       esc %>% st_drop_geometry %>%
         dplyr::select(`Categoría agregada`) %>% count(`Categoría agregada`) %>% 
         mutate(`Monto (US$)` = ifelse(`Categoría agregada` == 'idóneo', n*7000, n*35000)) %>% 
         adorn_totals())
get(obj) #Tabla
```

### Escenario 150 km\textsuperscript{2} por estación, eliminando propuestas de sitios redundantes respecto de ONAMET activas + INDRHI buenas y regulares

- Mapa

```{r}
indice <- 2; escenario <- '150'
redundancia <- 'activas_buenas_regulares'; estaciones <- actbuereg; distancia <- actbuereg_d
esc_d <- raster::extract(distancia, escenarios_ai_mi[[indice]])
esc <- escenarios_ai_mi[[indice]] %>%
  mutate(dist_onamet_indrhi = esc_d) %>% 
  filter(dist_onamet_indrhi > resumen_calculos_escenarios[[indice]]$`Distancia esperada entre vecinos`*1000) %>%
  st_join(all_criteria_scores_excluded, left = T)
esc %>%
  st_write(paste0('out/escenario_', escenario, '_km2_por_estacion_exclusion_redundancia_', redundancia, '.gpkg'), delete_dsn = T)
obj <- paste0('esc_', escenario, '_', redundancia, '_mapa')
assign(obj,
       bind_rows(esc %>% st_geometry %>% st_as_sf() %>%
                   mutate(id='sitios propuestos'),
                 estaciones %>% st_geometry %>% st_as_sf() %>%
                   mutate(id='estaciones existentes')) %>%
         ggplot +
         geom_sf(data = pais, fill = 'transparent', color = 'grey50') +
         geom_sf(alpha = 0.8, aes(fill = id, shape = id), size = 1.5) +
         scale_fill_manual(values = c('grey70', 'black')) +
         scale_shape_manual(values = c(25, 21)) +
         labs(title = paste0(trimws(names(escenarios)[indice]),'\n',
                             'Eliminación de redundancia respecto de estaciones ',
                             gsub('_', '+', redundancia))) +
         theme_bw() + 
         ggspatial::annotation_scale(style = 'ticks') +
         theme(legend.title = element_blank())) 
get(obj) #Mapa
```

- Tabla-resumen conteniendo los valores

```{r}
obj <- paste0('esc_', escenario, '_', redundancia, '_df_resumen')
assign(obj,
       esc %>% st_drop_geometry %>%
         dplyr::select(`Categoría agregada`) %>% count(`Categoría agregada`) %>% 
         mutate(`Monto (US$)` = ifelse(`Categoría agregada` == 'idóneo', n*7000, n*35000)) %>% 
         adorn_totals())
get(obj) #Tabla
```

### Escenario 250 km\textsuperscript{2} por estación, eliminando propuestas de sitios redundantes respecto de ONAMET activas + INDRHI buenas

- Mapa

```{r}
indice <- 3; escenario <- '250'
redundancia <- 'activas_buenas'; estaciones <- actbue; distancia <- actbue_d
esc_d <- raster::extract(distancia, escenarios_ai_mi[[indice]])
esc <- escenarios_ai_mi[[indice]] %>%
  mutate(dist_onamet_indrhi = esc_d) %>% 
  filter(dist_onamet_indrhi > resumen_calculos_escenarios[[indice]]$`Distancia esperada entre vecinos`*1000) %>%
  st_join(all_criteria_scores_excluded, left = T)
esc %>%
  st_write(paste0('out/escenario_', escenario, '_km2_por_estacion_exclusion_redundancia_', redundancia, '.gpkg'), delete_dsn = T)
obj <- paste0('esc_', escenario, '_', redundancia, '_mapa')
assign(obj,
       bind_rows(esc %>% st_geometry %>% st_as_sf() %>%
                   mutate(id='sitios propuestos'),
                 estaciones %>% st_geometry %>% st_as_sf() %>%
                   mutate(id='estaciones existentes')) %>%
         ggplot +
         geom_sf(data = pais, fill = 'transparent', color = 'grey50') +
         geom_sf(alpha = 0.8, aes(fill = id, shape = id), size = 1.5) +
         scale_fill_manual(values = c('grey70', 'black')) +
         scale_shape_manual(values = c(25, 21)) +
         # geom_sf(data = all_criteria_scores_excluded,
         #         aes(fill = `Categoría agregada`), lwd = 0, alpha = 0.4) +
         # scale_fill_manual(values = paleta) +
         labs(title = paste0(trimws(names(escenarios)[indice]),'\n',
                             'Eliminación de redundancia respecto de estaciones ',
                             gsub('_', '+', redundancia))) +
         theme_bw() + 
         ggspatial::annotation_scale(style = 'ticks') +
         theme(legend.title = element_blank())) 
get(obj) #Mapa
```

- Tabla-resumen conteniendo los valores

```{r}
obj <- paste0('esc_', escenario, '_', redundancia, '_df_resumen')
assign(obj,
       esc %>% st_drop_geometry %>%
         dplyr::select(`Categoría agregada`) %>% count(`Categoría agregada`) %>% 
         mutate(`Monto (US$)` = ifelse(`Categoría agregada` == 'idóneo', n*7000, n*35000)) %>% 
         adorn_totals())
get(obj) #Tabla
```

### Escenario 250 km\textsuperscript{2} por estación, eliminando propuestas de sitios redundantes respecto de ONAMET activas + INDRHI buenas y regulares

- Mapa

```{r}
indice <- 3; escenario <- '250'
redundancia <- 'activas_buenas_regulares'; estaciones <- actbuereg; distancia <- actbuereg_d
esc_d <- raster::extract(distancia, escenarios_ai_mi[[indice]])
esc <- escenarios_ai_mi[[indice]] %>%
  mutate(dist_onamet_indrhi = esc_d) %>% 
  filter(dist_onamet_indrhi > resumen_calculos_escenarios[[indice]]$`Distancia esperada entre vecinos`*1000) %>%
  st_join(all_criteria_scores_excluded, left = T)
esc %>%
  st_write(paste0('out/escenario_', escenario, '_km2_por_estacion_exclusion_redundancia_', redundancia, '.gpkg'), delete_dsn = T)
obj <- paste0('esc_', escenario, '_', redundancia, '_mapa')
assign(obj,
       bind_rows(esc %>% st_geometry %>% st_as_sf() %>%
                   mutate(id='sitios propuestos'),
                 estaciones %>% st_geometry %>% st_as_sf() %>%
                   mutate(id='estaciones existentes')) %>%
         ggplot +
         geom_sf(data = pais, fill = 'transparent', color = 'grey50') +
         geom_sf(alpha = 0.8, aes(fill = id, shape = id), size = 1.5) +
         scale_fill_manual(values = c('grey70', 'black')) +
         scale_shape_manual(values = c(25, 21)) +
         labs(title = paste0(trimws(names(escenarios)[indice]),'\n',
                             'Eliminación de redundancia respecto de estaciones ',
                             gsub('_', '+', redundancia))) +
         theme_bw() + 
         ggspatial::annotation_scale(style = 'ticks') +
         theme(legend.title = element_blank())) 
get(obj) #Mapa
```

- Tabla-resumen conteniendo los valores

```{r}
obj <- paste0('esc_', escenario, '_', redundancia, '_df_resumen')
assign(obj,
       esc %>% st_drop_geometry %>%
         dplyr::select(`Categoría agregada`) %>% count(`Categoría agregada`) %>% 
         mutate(`Monto (US$)` = ifelse(`Categoría agregada` == 'idóneo', n*7000, n*35000)) %>% 
         adorn_totals())
get(obj) #Tabla
```
